%\section{SARSA Algorithm}
\section{Reinforcement Learning}
%%%%% from IJCNN %%%%
Reinforcement learning is a reward-based learning algorithm that allows agents
to learn from experience. More formally, reinforcement
learning (RL) is a mathematical framework for learning from a reward signal that
is derived from Bellman's equation for optimal control \cite{sutton1998introduction}. One of
the most important forms of RL is temporal-difference (TD) RL. TD-RL is a method
for learning optimal behavior from changes in state and reinforcement by error
prediction \cite{sutton1988learning}. TD-RL agents learn an expected return that will be
received after taking an action in any state. Strong correlations with this type of
error predictive behavior have been found in studies of dopamine neurons
\cite{schultz1993responses}. This line of research has continued and is now been
supported by fMRI data of reward processing for tastes, money, and love
\cite{haber2009reward}.

TD-RL is used to solve Markov decision processes, which are an extension of
Markov chains to problems framed in terms of state, action, and reward.
Reward signals are learned and encoded
in a table which associates action preferences with states. The basic
TD($\gamma$) algorithm updates one state-action association at a time which
prohibits sequence learning. Eligibility traces are used to associate reward
with sequences of actions by reinforcing a weighted history of most recent
actions. In this study the online version of TD-RL, SARSA (short for,
state-action-reward-state-action), is used. A review of the nuances of
reinforcement learning can be found in \cite{sutton1998introduction}.

\subsection{SARSA Algorithm}

We include a few of the key equations from the SARSA algorithm. If we are in state,
$s_t$ at time $t$, then we will take some action $a_t$ which will bring us a
reward $r_t$. This action will also cause us to transition to a new state,
$s_{t+1}$. The SARSA algorithm learns a Q-function, which maps a value to each 
state-action pair, $(s_t,a_t)$. From each state multiple actions, $A_t$, may be taken
which may be a function of $s_t$ (for example, an obstacle may prevent an action
in a given state). Given an optimal Q-function the best action to take is
\begin{equation} argmin_{a_t \in A_t} Q(s_t,a_t).
\end{equation}
\noindent The Q-function is approximated by SARSA with the following update rule
\begin{eqnarray} 
& Q(s_t,a_t) \leftarrow Q(s_t,a_t)+ \nonumber \\ 
& \alpha \big[r_{t+1}+\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\big] 
\end{eqnarray}
\noindent where $\alpha$ is the learning rate, and $\gamma$ is the discounting
factor. Given only this update rule it can be difficult to compute the Q-value
for state-action pairs which indirectly contribute to obtaining a reward. This
update method propagates information only to the preceding state-action pair,
for those that are very distant from the reward, such as in the case of maze
solving problems, this can require a large number of repeated trials. However,
this problem of reward propagation can be partially alleviated by the use of
eligibility traces. Eligibility traces store an accumulating trace of
state-action pairs. The ``memory'' of these state-action pairs can be tuned with
the trace decay parameter $\lambda$. Eligibility traces are updated with
\begin{equation}
e_t(s,a) = 	\begin{cases}
			\gamma \lambda e_{t-1}(s,a) & \mbox{if } s \neq s_t \\ 
			\gamma \lambda e_{t-1}(s,a)+1 & \mbox{if }s = s_t \\
			\end{cases}
\end{equation} 
\noindent By combining the error predictive capabilities of SARSA with the
state-action sequence memory of eligibility traces we can amplify the effects
of our reward and speed up the learning process. When performing on-policy
learning it is important to ensure that a sufficient amount of exploration
occurs. To this end the $\epsilon$-greedy method is used, where a random
action is taken with $p(\epsilon)$, otherwise the agent's most preferred 
action is taken. However, the RL algorithm can still fail to capitalize on rarely
experienced rewards. 