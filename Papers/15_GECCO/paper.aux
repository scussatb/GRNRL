\relax 
\citation{Li2009}
\citation{Taylor2009}
\citation{Harrington2013}
\citation{sutton1998introduction}
\citation{sutton1988learning}
\citation{schultz1993responses}
\citation{sutton1998introduction}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {2}Reinforcement Learning}{\thepage }}
\newlabel{sec:RL}{{2}{\thepage }}
\citation{Davidson2006}
\citation{Joachimczak08}
\citation{Doursat09}
\citation{CussatBlanc2012a}
\citation{ziegler2001evolving}
\citation{Nicolau10}
\citation{Joachimczak10}
\citation{CussatBlanc2012b}
\citation{Banzhaf03}
\citation{Destexhe2004}
\citation{Marder2012}
\citation{Marder2002}
\citation{Schultz1993}
\citation{Fellous1998}
\citation{Marder2012}
\citation{Soltoggio2008}
\citation{Harrington2013}
\citation{Harrington2013}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gene Regulatory Network}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {4}Neuromodulation}{\thepage }}
\citation{Doya2002}
\citation{Schweighofer2003}
\citation{Doya2008}
\citation{stanley2002evolving}
\citation{cussatblanc2015grneat}
\citation{cussatblanc2015grneat}
\citation{Degris2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Regulating parameters}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces At every time step, SARSA updates the GRN inputs. The GRN returns updated learning parameters that will be used by the SARSA algorithm.}}{\thepage }}
\newlabel{fig:GRNSARSA}{{1}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}GRN Optimization}{\thepage }}
\citation{Moore1991}
\citation{Sutton1990}
\citation{Handa2007}
\citation{Boyan1995}
\citation{sutton1996generalization}
\citation{sutton1998introduction}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Mountain Car }}{\thepage }}
\newlabel{fig:MC:problem}{{2}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Maze }}{\thepage }}
\newlabel{fig:MZ:problem}{{3}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Puddle World }}{\thepage }}
\newlabel{fig:PW:problem}{{4}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Acrobat }}{\thepage }}
\newlabel{fig:ACP:problem}{{5}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces GRNEAT parameters used for evolution.}}{\thepage }}
\newlabel{tab:GREAT:params}{{1}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {5}Problems}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Mountain Car}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Maze}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Puddle World}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Acrobat}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Training GRN on one specific problem}{\thepage }}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Fixed learning parameters for the SARSA algorithm obtained by parameter sampling.}}{\thepage }}
\newlabel{tab:SARSAFixedParams}{{2}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Mountain car: (a) Convergence curve of GRNEAT (lower is better); (b) Regulation of the learning parameters of the best GRN obtained; (c) Comparison of the fitnesses per episode (abscissa) obtained by our neuromodulation with a GRN trained on mountain car (green), with a GRN trained on maze (blue) and with fixed-parameter SARSA algorithm (red). Results are averaged on 100 independent runs. Lower is better.}}{\thepage }}
\newlabel{fig:MC:Results}{{6}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Mountain Car}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Maze}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Puddle World}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9}Acrobat}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Maze: (a) GRN dynamics of genetically-regulated neuromodulation with the best GRN obtained; (b) Comparison of the fitnesses per episode (abscissa) obtained by GRNM with a GRN trained on Maze (green) and with fixed-parameter SARSA (red). Results are averaged on 100 independent runs. Lower is better.}}{\thepage }}
\newlabel{fig:MZ:Results}{{7}{\thepage }}
\citation{sanchez2014gene}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Detailed results of the 100 runs from the four problems with the best GRN trained on the specific problem, the SARSA algorithm with fixed parameters, the GRN trained on Maze (noted GRN$_{m}$ in the table), and a generic GRN trained both on the Maze and Mountain Car problems (noted GRN$_{g}$). Notice that, for the first two problems (Mountain Car and Maze), lower is better and for the two others (Puddle World and Acrobat), higher is better. Bold values are best per problem, per row. Italics indicate second best.}}{\thepage }}
\newlabel{tab:results}{{3}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {6}Using the GRN trained on Maze}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {7}Evolving a generic GRN}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Puddle World: (b) Comparison of the fitnesses per episode (abscissa) obtained by genetically-regulated neuromodulation with a GRN trained on this problem (green), a GRN trained on Maze (blue), a GRN trained on Maze and Mountain Car (gray) and with the fixed-parameter SARSA algorithm (red). Results are averaged on 100 independent runs. Higher is better.}}{\thepage }}
\newlabel{fig:PW:Results}{{8}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Acrobat: (a) Regulation of the learning parameters of the best GRN obtained; (b) Comparison of the fitnesses per episode (abscissa) obtained by genetically-regulated neuromodulation with a GRN trained on this problem (green) and a GRN trained on Maze (blue) and with the fixed-parameter SARSA algorithm (red). Results are averaged on 100 independent runs. Higher is better. }}{\thepage }}
\newlabel{fig:ACP:Results}{{9}{\thepage }}
\citation{Schweighofer2003}
\citation{Gold2014}
\bibstyle{abbrv}
\bibdata{bibtex}
\bibcite{Banzhaf03}{1}
\bibcite{Boyan1995}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Behavior of the GRN trained on the Maze problem when used to regulate learning parameters on Mountain Car, Puddle World and Acrobat.}}{\thepage }}
\newlabel{fig:all:GRNMazeBehavior}{{10}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {9}References}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Behavior of the generic GRN when used to regulate learning parameters on all problems.}}{\thepage }}
\newlabel{fig:GRNGenericBehavior}{{11}{\thepage }}
\bibcite{cussatblanc2015grneat}{3}
\bibcite{CussatBlanc2012a}{4}
\bibcite{CussatBlanc2012b}{5}
\bibcite{Davidson2006}{6}
\bibcite{Degris2014}{7}
\bibcite{Destexhe2004}{8}
\bibcite{Doursat09}{9}
\bibcite{Doya2002}{10}
\bibcite{Doya2008}{11}
\bibcite{Fellous1998}{12}
\bibcite{Gold2014}{13}
\bibcite{Handa2007}{14}
\bibcite{Harrington2013}{15}
\bibcite{Joachimczak08}{16}
\bibcite{Joachimczak10}{17}
\bibcite{Li2009}{18}
\bibcite{Marder2012}{19}
\bibcite{Marder2002}{20}
\bibcite{Moore1991}{21}
\bibcite{Nicolau10}{22}
\bibcite{sanchez2014gene}{23}
\bibcite{schultz1993responses}{24}
\bibcite{Schultz1993}{25}
\bibcite{Schweighofer2003}{26}
\bibcite{Soltoggio2008}{27}
\bibcite{stanley2002evolving}{28}
\bibcite{Sutton1990}{29}
\bibcite{sutton1988learning}{30}
\bibcite{sutton1996generalization}{31}
\bibcite{sutton1998introduction}{32}
\bibcite{Taylor2009}{33}
\bibcite{ziegler2001evolving}{34}
