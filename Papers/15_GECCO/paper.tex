\documentclass{sig-alternate-2}
   
   \usepackage{dblfloatfix}
   
 \conferenceinfo{GECCO'15,} {July 11-15, 2015, Madrid, Spain.}
 \CopyrightYear{2015}
 \crdata{TBA}
 \clubpenalty=10000
 \widowpenalty = 10000
    
    \title{Genetically-regulated Neuromodulation\\
    Facilitates Multi-Task Reinforcement Learning}
    
\numberofauthors{2}
\author{
\alignauthor
Sylvain Cussat-Blanc\\
\affaddr{University of Toulouse}\\
\affaddr{21 All\'ee de Brienne} \\
\affaddr{31042 Toulouse, France}
\email{cussat@irit.fr}
\and
\alignauthor
Kyle I. S. Harrington\\
\affaddr{Beth Israel Deaconess Medical Center}\\
\affaddr{Harvard Medical School}\\
\affaddr{02215 Boston, MA} \\
\email{kharrin3@bidmc.harvard.edu}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
In this paper, we use a gene regulatory network (GRN) to regulate a reinforcement learning controller, the State-Action-Reward-State-Action (SARSA) algorithm. The GRN serves as neuromodulator of SARSA's learning parameters: learning rate, discount factor, and memory depth. We have optimized GRNs with an evolutionary algorithm to regulate these parameters on specific problems but with no knowledge of problem structure. We show that genetically-regulated neuromodulation (GRNM) performs comparably or better than SARSA with fixed parameters. We then extend the GRNM SARSA algorithm to multi-task problem generalization, and show that GRNs optimized on multiple problem domains can generalize to previously unknown problems with no further optimization. 
\end{abstract}

\category{I.2.6}{Learning}{Parameter learning}
\keywords{Reinforcement learning, Gene regulatory network, Parameter control, Multi-task Learning, Neuromodulation}

\input{01_introduction}

\input{02_SARSA}

\input{03_GRN}

\input{04_GRNSARSA}

\input{05_experiences}

\input{06_conclusion}

\bibliographystyle{abbrv}
\bibliography{bibtex}


\end{document}

