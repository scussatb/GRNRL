%%% Problem figures are in GRNvsSARSA file %%%
\section{Experiments}

% KH: Parameter sampling for SARSA. Range sampled over $\alpha$, $\gamma$, and $\lambda$. What $\epsilon$ was used.
% SCB: See the begining of the result part

\subsection{Problems}

We have studied GRNM on four classical problems: mountain car, maze, puddle world and acrobat. In all cases agents do not have prior information about the specific task. Before detailling the results obtained with GRNM, this section presents each problems.
\subsubsection{Mountain Car}
The mountain car problem was introduced by Moore to help broad applicability of dynamic programming \cite{Moore1991}. In the mountain car problem, depicted by figure \ref{fig:MC:problem}, the agent is placed at the bottom of a valley with no initial velocity and must drive to the top of the right hill. The agent can take one of 3 actions: full thrust forward, full thrust in reverse, or no thrust. The difficulty of the task lies in the fact that the right hill is too steep to be climbed at full thrust, and thus the agent must first reverse up the left hill to gain sufficient momentum to climb the right hill. The agent experiences a negative reward for all time steps that it has not reached the top of the right hill, at which point the episode is completed. In the GRNM architecture and especially during the optimization stage, the fitness of a training run is given by the number of step necessary to complete a series of 25 episodes.

\subsubsection{Maze}

The maze task was originally introduced by Sutton as an example problem for demonstrating the capabilities of initial reinforcement learning algorithms \cite{Sutton1990}. The maze is a 6 by 9 grid with 7 obstacles, a start, and a goal position. From any position, the agent may take any of 4 actions (right, left, up, and down); however, when the action would move the agent out-of-bounds or onto an obstacle, the agent remains in place. The only time a reward is received is when the agent reaches the goal, at which point the episode is completed. Although this problem and its variants have been studied by a number of groups, of particular interest is the Hanada's study which applies evolutionary programming to a multi-task version of Sutton's maze \cite{Handa2007}. The fitness of a training run is given by the number of step necessary to complete a series of 30 episodes.

%Used to test RL in sparse coding \cite{sutton1996generalization}

\subsubsection{Puddle World}

The puddle world problem was also introduced by Boyan and Moore \cite{Boyan1995}, but was later presented in greater detail by Sutton \cite{sutton1996generalization}. Agents solving the puddle world task exist in a 2D continuous space. Agents may take 4 actions as in the maze problem (left, right, up, and down), but as opposed to the maze problem, actions in puddle world are stochastic. For these experiments the world is 100 by 100, and an agent action moves a unit distance $\pm$ noise taken uniformly from [-0.1,0.1]. The agent experiences a reward of -1 for each timestep until the episode is complete, and, if within either puddle, $-400*d$, where $d$ is the closest distance to a puddle's edge. The fitness is given by the aggregation of the rewards received during 40 episodes.

%Used to test RL in sparse coding \cite{sutton1996generalization}

\begin{figure*}[t!]
\center
\begin{tabular}{ccc}
\includegraphics[height=2.7cm]{MC_convergence.pdf} &
\includegraphics[height=2.7cm]{MC_GRNBehavior.pdf} &
\includegraphics[height=2.7cm]{MC_GRNvsSARSA.pdf}\\
(a) Convergence curve &
(b) Best GRN inputs and outputs &
(c) Neuromodulation vs SARSA
\end{tabular}
\caption{Mountain car: (a) Convergence curve of the genetic algorithm (lower is better); (b) Regulation of the learning parameters of the best GRN obtained; (c) Comparison of the fitnesses per episode (abscissa) obtained by our neuromodulation with a GRN trained on mountain car (green), with a GRN trained on maze (blue) and with parameter-fixed SARSA (red). Results are averaged on 100 independent runs. Lower is better.}\label{fig:MC:Results}
\end{figure*}

\begin{figure*}[b!]
\center
\begin{tabular}{cc}
\includegraphics[height=3cm]{MZ_GRNBehavior.pdf}&
\includegraphics[height=3cm]{MZ_GRNvsSARSA.pdf}\\
(a) Behavior of the best GRN &
(b) Neuromodulation vs SARSA
\end{tabular}
\caption{Maze: (a) Regulation of the learning parameters of the besst GRN obtained; (b) Comparison of the fitnesses per episode (abscissa) obtained by our neuromodulation with a GRN trained on maze (green) and with parameter-fixed SARSA (red). Results are averaged on 100 independent runs. Lower is better.}\label{fig:MZ:Results}
\end{figure*}


\subsubsection{Acrobat}

The acrobat problem has a long-standing history in robotics and control, some references to which may be found in \cite{sutton1998introduction}. The agent controls 2 segments with 2 joints, where the first joint represents attachment to the bar, and the second joint represents the waist of the acrobat. The agent can only control the second joint, and the corresponding actions are: apply a fixed amount of positive torque, apply a fixed amount of negative torque, or apply no torque. The agent is initialized in a downward vertical position and must reach an upward vertical position. The agent receives a reward of -1 for each timestep until the episode is complete. The fitness is given by the aggregation of the rewards received during 30 episodes.


\subsection{Training GRN on one specific problem}
In this first experience, we have trained our GRN-based neuromodulation model independently on each problem above mentionned. To evaluate the gain provided by neuromodulation, we first determine the best fixed learning parameter to use with SARSA on each problem. We use parameter sampling on $\alpha$, $\gamma$ and $\lambda$ in $[0, 1]$ with 0.0714 steps (15 evaluations). Each evaluation is averaged on 10 replicates in order to reduce the randomness of the problems. At the end of the parameter sampling stage, we chose the best fixed learning parameters for a given problem by selecting the tuple with the highest fitness. These parameters are given in table \ref{tab:SARSAFixedParams}.

\begin{table}[h]
\center
\begin{tabular}{|c|ccc|}
\cline{2-4}
\multicolumn{1}{c|}{ }	& $\alpha$	& $\gamma$	& $\lambda$	\\\hline
Mountain car			& 0.071429	& 1.0		& 0.928571 	\\%\hline
Maze				& 1.0		& 0.928571	& 0.928571	\\%\hline
Puddle world			&  0.057142	& 0.928571	& 0.5		\\%\hline
Acrobate				& 0.05		& 0.928571	& 0.785714	\\\hline
\end{tabular}
\caption{Fixed learning parameters for SARSA obtained by parameter sampling}\label{tab:SARSAFixedParams}
\end{table}

\subsubsection{Mountain car}
Firstly, the GRN is trained on Mountain car. Figure \ref{fig:MC:Results}(a) shows the convergence curve of the genetic algorithm on this particular problem. We can observe that the best GRN is found very quickly (green curve) before being slowly optimized to regulate the learning parameters more efficiently. Figure \ref{fig:MC:Results}(b) plots the behavior of the best GRN obtained after 150 iterations. This figure plots the input protein concentrations and the three learning parameters calculated by the gene regulatory network. The GRN maintains the parameter values almost constant over the time except at the beginning of each episode where $\alpha$ and $\gamma$ are increase to explore more, certainly due to the novelty of the environment. In this experience, it has also to be noticed that $\lambda$ is kept to zero all over time. On this simple problem, memory might not be necessary. 

This GRN is then compared to the SARSA algorithm with fixed learning parameters on 100 independent reruns of mountain car with different different random seeds. The point is to evaluate the capacity of both approaches to handle the noise generated by randomness. Figure \ref{fig:MC:Results}(c) shows the results obtained for each episode averaged on the 100 runs. We can observe that neuromodulation with GRN trained on the mountain car (in green) beats SARSA with fixed parameters (in red) on every episodes. More specifically, the first episodes (from 1 to 3) show that GRNM learns faster than SARSA and later episodes (from 3 to 25) show that neuromodulation solves the problem faster than SARSA. When comparing detailed results on table \ref{tab:results}, the GRN-based neuromodulation gives better results on the mountain car problem when averaged on 100 independent runs. 

\subsubsection{Maze}
The same procedure has been used to evolve a gene regulatory network to regulate SARSA's learning parameters on the maze problems. Figure \ref{fig:MZ:Results}(a) presents the behavior of the best GRN obtained after evolution. The regulation broadly differs from the one obtained on the mountain car problem. This GRN starts with very high values for all learning parameters with the aim to explore widely the environment. Then, with sucessfull episodes incoming, the learning rate $\alpha$ and the memory depth $\lambda$ decrease to exploit the behavior learnt during the first scenarios. $\gamma$ is kept very high all along the simulation.

When compared to parameter-fixed SARSA (figure \ref{fig:MZ:Results}(b)), we can observe that GRNM is learning faster than SARSA (episodes 2 to 5) due to its high learning rate at the begining of the simulation. However, the avantage turns to fixed-parameter SARSA in the remaining episodes: SARSA is doing slightly better than neuromodulation from episode 6 to the end. When refering to the detailed results given by table \ref{tab:results}, neuromodulation and SARSA give comparable results. 


\begin{figure*}[t!]
\center
\begin{tabular}{cc}
\includegraphics[height=3cm]{PW_GRNBehavior.pdf} &
\includegraphics[height=3cm]{PW_GRNvsSARSA.pdf} \\
(a) Behavior of the best GRN &
(b) Neuromodulation vs SARSA
\end{tabular}
\caption{Puddle world: (b) Comparison of the fitnesses per episode (abscissa) obtained by our neuromodulation with a GRN trained on this problem (green), a GRN trained on maze (blue), a GRN trained on maze and mountain car (gray) and with parameter-fixed SARSA (red). Results are averaged on 100 independent runs. Higher is better.}\label{fig:PW:Results}
\end{figure*}

\begin{figure*}[b!]
\center
\begin{tabular}{cc}
\includegraphics[height=3cm]{ACP_GRNBehavior.pdf} &
\includegraphics[height=3cm]{ACP_GRNvsSARSA.pdf} \\
(a) Behavior of the best GRN &
(b) Neuromodulation vs SARSA
\end{tabular}
\caption{Acrobat: (a) Regulation of the learning parameters of the besst GRN obtained; (b)Comparison of the fitnesses per episode (abscissa) obtained by our neuromodulation with a GRN trained on this problem (green) and a GRN trained on maze (blue) and with parameter-fixed SARSA (red). Results are averaged on 100 independent runs. Higher is better. }\label{fig:ACP:Results}
\end{figure*}

\subsubsection{Puddle world}
Figure \ref{fig:PW:Results}(a) presents the behavior of the best GRN obtained when neuromodulation is optimized on the puddle world problem. Once again, the GRN choses to use higher learning parameters $\alpha$ and $\lambda$ at the begining of the simulation in order to explore the environment and memorize the rules faster. When done, the GRN increases the discount factor and reduce the exploration and memorization values in order to exploit more. This gives good results in comparison to SARSA with no neuromodulation: as depicted on figure \ref{fig:PW:Results}(b), neuromodulation learns faster the rules to solve the problem and converge to an equivalent solution than SARSA. When comparing the results in detail (table \ref{tab:results}), neuromodulation obtains equivalent results when averaged on 100 independent runs but the standard deviation and worst result are better: the GRNM is capable to modulate the learning rates when harder scenarios are met. 

\subsubsection{Acrobat}

Figure \ref{fig:ACP:Results}(a) shows the regulation obtained with the best GRN evolved on the acrobat problem. The GRN finds learning parameters very close to the one obtained with parameter sampling for parameter-fixed SARSA. However, in addition to finding these parameters, the GRN reduce these values all along the episodes, in particular $\alpha$ which is equal to zero at the end. As in the puddle world problem, the aim is to exploit more the results when an appropriate behavior is found by SARSA. When compared to parameter-fixed SARSA on this problem (figure \ref{fig:ACP:Results}(b)), GRN-based neuromodulation both learns faster and finishes with a better behavior than parameter-fixed SARSA. This is confirmed by table \ref{tab:results} in which the reward averaged on 100 independent runs is largely over with neuromodulation than with parameter-fixed SARSA. 

\subsection{Generalization of the regulation}
\subsubsection{Using the GRN trained on the maze}
One of the main property of the gene regulatory network is its capacity to generalize its behavior to unknown situations \cite{sanchez2014gene}. In this neuromodulation problem, we noticed that the best GRN trained on the maze problem and used on other problems good provides results in comparison to SARSA with no neuromodulation. As presented in table \ref{tab:results}, GRNM trained on maze (noted GRN$_{m}$) obtains better results than parameter-fixed SARSA on all problems we have tested. In our opinion, the regulation dynamics are very generic: when the rewards start to increase, lowering the learning rate to enter an exploitation phase seams appropriate to most problems.

However, this generic regulatory network does not beat GRNs specificially trained on the problems. This can be explained by the fact that optimizing the GRN specifically on a problem allows the system to find particular artifacts and thus improves the quality of the parameter regulation as well as the best initial values.

Figure \ref{fig:all:GRNMazeBehavior} shows the behavior of the GRN trained on the maze and used on other problems. The behavior are slightly different on each problem: the GRN is able to adapt to the problem specificities. In particular, when the episodes are taking to much time, this GRN increases $\lambda$ first and then $\alpha$. It is also interesting to notice that the stabilized values of the learning parameters different from a problem to another. For example, $\lambda$ is kept around 0.8 on the mountain car problem, around 0.65 for the puddle world and the acrobat problems whereas it is equal to 0.7 on the maze problem. It shows the capacity of the GRN to approximate the best learning parameters to a problem without further learning.

When compared to SARSA and neuromodulation with GRN trained specifically on one problem (table \ref{tab:results}), the GRN evolved on the maze is doing better than SARSA on mountain car and acrobat problems but have poorer results on puddle world. The worst reward obtained is better than parameter-fixed SARSA in all problem, even on puddle world where the average reward is lower. Once again, this shows the capacity of neuromodulation to balance the learning parameters in harder scenarios.

\begin{table*}[t!]
\center
\setlength{\tabcolsep}{.5mm}
\begin{tabular}{|c|cccc|ccc|cccc|cccc|cccc|}
\cline{2-16}
\multicolumn{1}{c}{ }	& \multicolumn{4}{|c}{Mountain car}	& \multicolumn{3}{|c}{Maze}	 			& \multicolumn{4}{|c}{Puddle world}				& \multicolumn{4}{|c|}{Acrobat} \\
\multicolumn{1}{c|}{ }	
		& GRN	& SARSA	& GRN$_{m}$	& GRN$_{g}$		& GRN$_{m}$	& SARSA	& GRN$_{g}$		& GRN & SARSA & GRN$_{m}$ & GRN$_{g}$		& GRN & SARSA & GRN$_{m}$ & GRN$_{g}$ 			\\\hline
avg		& \textbf{170.5} & 294.2 & \emph{225.3} & 228.3 	& \emph{56.8} & \textbf{53.2} & 83.5	& \textbf{661.9} & \emph{639.0} & 533.7 & 569.5	& \textbf{420.1} & 231.86 & 320.0 & \emph{330.0}		\\
stdev	& \emph{16.7} & \textbf{4.1} & 17.2 & 41.6	  	& \emph{6.1}	& \textbf{4.3}	& 8.4 		& \textbf{105.2} & 234.7 & 177.6 & \emph{175.2}	& \textbf{14.7} & 127.9 & \emph{37.2} & 57.9			\\
best		& \textbf{152.3} & 286.0 & 191.0 & \emph{183.9} 	& \emph{44.0} & \textbf{43.7} & 68.1	& 728.7& \textbf{907.1} & 728.7 & \emph{757.1}	& \emph{453.2} & \textbf{555.0} & 376.7& 404.2		\\
worst	&\textbf{227.4} & 308.1 & \emph{269.6} & 573.2	& \emph{73.2} & \textbf{73.1} & 127.2	& \textbf{281.1}	& -49.5 & 164.6 &	\emph{183.8}	& \textbf{382.2} & 93.5 & \emph{156.8}& 149.3		\\\hline
\end{tabular}
\caption{Detailed results of the 100 runs done on the four problems with the best GRN specifically trained on the problem, with SARSA with no neuromodulation, with the GRN trained on the maze (noted GRN$_{m}$ in the table), and a generic GRN trained both on the maze and the mountain car problems (noted GRN$_{g}$). Notice that, for the first two problems (mountain car and maze), lower is better and for the two others (puddle world and acrobat), higher is better. Bold values are best per problem and per row and italic ones are second best.}\label{tab:results}
\end{table*}

\begin{figure*}[t!]
\center
\begin{tabular}{ccc}
\includegraphics[width=0.32\linewidth]{MC_GRNMazeBehavior.pdf} &
\includegraphics[width=0.32\linewidth]{PW_GRNMazeBehavior.pdf} &
\includegraphics[width=0.32\linewidth]{ACP_GRNMazeBehavior.pdf} \\
(a) Mountain Car & (b) Puddle world & (c) Acrobat
\end{tabular}
\caption{Behavior of the GRN trained on the maze when used to regulate learning parameters on mountain car, puddle world and acrobat.}\label{fig:all:GRNMazeBehavior}
\end{figure*}

\subsubsection{Evolving a generic GRN}
Based on this observation, we have trained a new GRN on two problems instead of only one. The aim is to obtain a generic GRN that have seen two problems instead of one. Therefore, we have chosen to train the GRN on the maze and the mountain car problems. Figure \ref{fig:GRNGenericBehavior} shows the behavior of the best GRN obtained. Comparable regulation behaviors can be observed on the different problems: at the begining of each episode, $\lambda$ is set up to a high value and then reduces until the end of the episode. However, the parameter values are regulated to different levels according to the problem. The GRN might use the current step quality and the average state reward inputs to regulate these levels. These inputs provide the GRN an estimation of the agent progression in the episode, in other words how well or bad the agent is currently doing, and, consequently, drive the GRN to set SARSA up to explore more to find better behavior rules.

To compare the generic GRN to specific GRNs and to the GRN trained on the maze problem, we have compared this GRN following the same procedure as in past problems: the generic GRN is evaluated on 100 independent runs with different random seeds. Its result are reported in table \ref{tab:results} within the GRN$_g$ column. It is interesting to notice that the generic GRN beats the GRN$_m$ on the puddle world and the acrobat problems but doesn't beat the GRNs specifically trained on these problems. The generic GRN and the GRN trained on maze are doing comparably on the mountain car problem, but the GRN trained on the maze is doing better on the maze. This was expected since the GRN trained on the maze is specifically trained on this problem whereas the generic GRN has been trained on two problems and thus its training is also impacted by the second problem. Just as the GRN trained on the maze, the generic GRN beats parameter-fixed SARSA on the mountain car and the acrobat problems. 

\begin{figure*}[t!]
\center
\setlength{\tabcolsep}{1mm}
\begin{tabular}{cccc}
\includegraphics[width=0.24\linewidth]{MC_GRNGenericBehavior.pdf} &
\includegraphics[width=0.24\linewidth]{MZ_GRNGenericBehavior.pdf} &
\includegraphics[width=0.24\linewidth]{PW_GRNGenericBehavior.pdf} &
\includegraphics[width=0.24\linewidth]{ACP_GRNGenericBehavior.pdf} \\
(a) Mountain Car & (b) Maze & (c) Puddle world & (d) Acrobat
\end{tabular}
\caption{Behavior of the GRN trained on the maze and mountain car when used to regulate learning parameters on other problems.}\label{fig:GRNGenericBehavior}
\end{figure*}

%Generalizing from which problems to which other problems. Does training on stochastic and testing on deterministic lead to better GRNs than training on
%deterministic then testing on stochastic?

% discrete problems seem harder for the GRN...
