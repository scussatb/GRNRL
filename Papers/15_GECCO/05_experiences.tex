\section{Experiments}

Parameter sampling for SARSA. Range sampled over $\alpha$, $\gamma$, and $\lambda$. What $\epsilon$ was used.

\subsection{Problems}

In all cases agents do not have prior information about the specific task.

\subsubsection{Maze}
\begin{figure}[h]
\center
\includegraphics[width=0.75\linewidth]{MZ_problem.pdf}
\caption{In the maze problem, the agent's task is to find its way in a maze from start (in red) to finish (in green). The agent receives a reward only when it escapes.}\label{fig:MC:problem}
\end{figure}

The maze task was originally introduced by Sutton as an example problem for demonstrating the capabilities of initial reinforcement learning algorithms \cite{Sutton1990}. The maze is a 6 by 9 grid with 7 obstacles, a start, and a goal position. From any position, the agent may take any of 4 actions (right, left, up, and down); however, when the action would move the agent out-of-bounds or onto an obstacle, the agent remains in place. The only time a reward is received is when the agent reaches the goal, at which point the episode is completed. Although this problem and its variants have been studied by a number of groups, of particular interest is the Hanada's study which applies evolutionary programming to a multi-task version of Sutton's maze \cite{Handa2007}.

\subsubsection{Mountain Car}
\begin{figure}[h]
\center
\includegraphics[width=0.75\linewidth]{MC_problem.pdf}
\caption{In the mountain car problem, a car must escape from a valley. It cannot climb the hill without moving backward first to gain speed. The agent receives a reward only when it escapes.}\label{fig:MC:problem}
\end{figure}

The mountain car problem was introduced by Moore to help broad applicability of dynamic programming \cite{Moore1991}. In the mountain car problem, the agent is placed at the bottom of a valley with no initial velocity and must drive to the top of the right hill. The agent can take one of 3 actions: full thrust forward, full thrust in reverse, or no thrust. The difficulty of the task lies in the fact that the right hill is too steep to be climbed at full thrust, and thus the agent must first reverse up the left hill to gain sufficient momentum to climb the right hill. The agent experiences a negative reward for all time steps that it has not reached the top of the right hill, at which point the episode is completed.

%Used to test RL in sparse coding \cite{sutton1996generalization}

\subsubsection{Puddle World}
\begin{figure}[h]
\center
\includegraphics[width=0.75\linewidth]{PW_problem.pdf}
\caption{In the puddle world problem, the agent must navigate without sensor information with stochastic movements from start (in red) to finish (in green). Local rewards are given all along its exploration and when it escapes.}\label{fig:MC:problem}
\end{figure}

The puddle world problem was also introduced by Boyan and Moore \cite{Boyan1995}, but was later presented in greater detail by Sutton \cite{sutton1996generalization}. Agents solving the puddle world task exist in a 2D continuous space. Agents may take 4 actions as in the maze problem (left, right, up, and down), but as opposed to the maze problem, actions in puddle world are stochastic. For these experiments the world is 100 by 100, and an agent action moves a unit distance $\pm$ noise taken uniformly from [-0.1,0.1]. The agent experiences a reward of -1 for each timestep until the episode is complete, and, if within either puddle, $-400*d$, where $d$ is the closest distance to a puddle's edge.

%Used to test RL in sparse coding \cite{sutton1996generalization}

\subsubsection{Acrobat}

\begin{figure}[h]
\center
\includegraphics[width=0.5\linewidth]{ACP_problem.pdf}
\caption{In the acrobat problem, the agent has to balance a pendulum in a perfectly upright position. The reward is given by the cosine of the angle between the pendulum and the resting position.}\label{fig:MC:problem}
\end{figure}

The acrobat problem has a long-standing history in robotics and control, some references to which may be found in \cite{sutton1998introduction}. The agent controls 2 segments with 2 joints, where the first joint represents attachment to the bar, and the second joint represents the waist of the acrobat. The agent can only control the second joint, and the corresponding actions are: apply a fixed amount of positive torque, apply a fixed amount of negative torque, or apply no torque. The agent is initialized in a downward vertical position and must reach an upward vertical position. The agent receives a reward of -1 for each timestep until the episode is complete.

\subsection{Training GRN on one specific problem}
In this first experience, we have trained our GRN-based neuromodulation model independently on each problem above mentionned. To evaluate the gain provided by neuromodulation, we first determine the best fixed learning parameter to use with SARSA on each problem. We use parameter sampling on $\alpha$, $\gamma$ and $\lambda$ in $[0, 1]$ with 0.0714 steps (15 evaluations). Each evaluation is averaged on 10 replicates in order to reduce the randomness of the problems. At the end of the parameter sampling stage, we chose the best fixed learning parameters for a given problem by selecting the tuple with the highest fitness. These parameters are given in table \ref{tab:SARSAFixedParams}.

\begin{table}[h]
\begin{tabular}{|c|ccc|}
\cline{2-4}
\multicolumn{1}{c|}{ }	& $\alpha$	& $\gamma$	& $\lambda$	\\\hline
Mountain car			& 0.071429	& 1.0		& 0.928571 	\\%\hline
Maze				& 1.0		& 0.928571	& 0.928571	\\%\hline
Puddle world			&  0.057142	& 0.928571	& 0.5		\\%\hline
Actor critic pendulum	& 0.05		& 0.928571	& 0.785714	\\\hline
\end{tabular}
\caption{Fixed learning parameters for SARSA obtained by parameter sampling}\label{tab:SARSAFixedParams}
\end{table}

\subsubsection{Mountain car}
First, the GRN is trained on Mountain car. Figure \ref{fig:MC:Convergence} shows the convergence curve of the genetic algorithm on this particular problem. We can observe that the best GRN is found very quickly (green curve) before being slowly optimized to regulate the learning parameters more efficiently. Figure \ref{fig:MC:GRNBehavior} plots the behavior of the best GRN obtained after 150 iterations. This figure plots the input protein concentrations and the three learning parameters calculated by the gene regulatory network. The GRN maintains the parameter values almost constant over the time except at the beginning of each episode where $\alpha$ and $\gamma$ are increase to explore more, certainly due to the novelty of the environment. In this experience, it has also to be noticed that $\lambda$ is kept to zero all over time. On this simple problem, memory might not be necessary. 

This GRN is then compared to the SARSA algorithm with fixed learning parameters on 100 independent reruns of mountain car with different different random seed. The point is to evaluate the capacity of both approaches to handle the noise generated by randomness. Figure \ref{fig:MC:GRNvsSARSA} shows the results obtained for each episode averaged on the 100 runs. We can observe that neuromodulation with GRN trained on the mountain car (in green) beats SARSA with fixed parameters (in red) on every episodes. More specifically, the first episodes (from 1 to 3) show that GRN-based neuromodulation learns faster than SARSA and later episodes (from 3 to 25) show that neuromodulation solves the problem faster than SARSA.

\begin{figure}[h]
\center
\includegraphics[width=\linewidth]{MC_GRNBehavior.pdf}
\caption{Moutain car: Regulation of the learning parameters of the best GRN obtained.}\label{fig:MC:GRNBehavior}
\end{figure}

\begin{figure}[h]
\center
\includegraphics[width=0.75\linewidth]{MC_convergence.pdf}
\caption{Mountain car: Convergence curve of the genetic algorithm (lower is better - time to escape the valley).}\label{fig:MC:Convergence}
\end{figure}

\begin{figure}[h]
\center
\includegraphics[width=\linewidth]{MC_GRNvsSARSA.pdf}
\caption{Mountain car: Comparison of the fitnesses per episode (abscissa) obtained by our neuromodulation with a GRN trained on mountain car (green), with a GRN trained on maze (blue) and with parameter-fixed SARSA (red). Results are averaged on 100 independent runs. Lower is better}\label{fig:MC:GRNvsSARSA}
\end{figure}

\subsubsection{Maze}
The same procedure has been used to evolve a gene regulatory network to regulate SARSA's learning parameters on the maze problems. Figure \ref{fig:MZ:GRNBehavior} presents the behavior of the best GRN obtained after evolution. The regulation broadly differs from the one obtained on the mountain car problem. This GRN starts with very high values for all learning parameters with the aim to explore widely the environment. Then, with sucessfull episodes incoming, the learning rate $\alpha$ and the memory depth $\lambda$ decrease to exploit the behavior learnt during the first scenarios. $\gamma$ is kept very high all along the simulation.

When compared to parameter-fixed SARSA (figure \ref{fig:MZ:GRNvsSARSA}), we can observe that the GRN is learning faster than SARSA (episodes 2 to 5) due to its high learning rate at the begining of the simulation. However, the avantage turns to fixed-parameter SARSA in the remaining episodes: SARSA is doing slightly better than neuromodulation from episode 6 to the end.

\begin{figure}[h]
\center
\includegraphics[width=\linewidth]{MZ_GRNBehavior.pdf}
\caption{Maze: Regulation of the learning parameters of the besst GRN obtained.}\label{fig:MZ:GRNBehavior}
\end{figure}

\begin{figure}[h]
\center
\includegraphics[width=\linewidth]{MZ_GRNvsSARSA.pdf}
\caption{Maze: Comparison of the fitnesses per episode (abscissa) obtained by our neuromodulation with a GRN trained on maze (green) and with parameter-fixed SARSA (red). Results are averaged on 100 independent runs. Lower is better.}\label{fig:MC:GRNvsSARSA}
\end{figure}


\subsubsection{Puddle world}
\begin{figure}[h]
\includegraphics[width=\linewidth]{PW_GRNvsSARSA.pdf}
\caption{Puddle world: Comparison of the fitnesses per episode (abscissa) obtained by our neuromodulation with a GRN trained on this problem (green), a GRN trained on maze (blue), a GRN trained on maze and mountain car (gray) and with parameter-fixed SARSA (red). Results are averaged on 100 independent runs. Higher is better.}\label{fig:PW:GRNvsSARSA}
\end{figure}



\subsubsection{Actor critic pendulum}
\begin{figure}[h]
\center
\includegraphics[width=\linewidth]{ACP_GRNBehavior.pdf}
\caption{Actor critic pendulum: Regulation of the learning parameters of the besst GRN obtained.}\label{fig:ACP:GRNBehavior}
\end{figure}

\begin{figure}[h]
\includegraphics[width=\linewidth]{ACP_GRNvsSARSA.pdf}
\caption{Actor critic pendulum: Comparison of the fitnesses per episode (abscissa) obtained by our neuromodulation with a GRN trained on this problem (green) and a GRN trained on maze (blue) and with parameter-fixed SARSA (red). Results are averaged on 100 independent runs. Higher is better.}\label{fig:ACP:GRNvsSARSA}
\end{figure}

Number of GRN runs, number of generations

\subsection{Generalization of the regulation}
\begin{table*}[t!]
\center
\setlength{\tabcolsep}{.5mm}
\begin{tabular}{|c|cccc|ccc|cccc|cccc|cccc|}
\cline{2-16}
\multicolumn{1}{c}{ }	& \multicolumn{4}{|c}{Mountain car}	& \multicolumn{3}{|c}{Maze}	 & \multicolumn{4}{|c}{Puddle world}	& \multicolumn{4}{|c|}{Actor critic pendulum} \\
\multicolumn{1}{c|}{ }	
		& GRN	& SARSA	& GRN$_{m}$	& GRN$_{g}$		& GRN$_{m}$	& SARSA	& GRN$_{g}$		& GRN	& SARSA	& GRN$_{m}$ 		& GRN$_{g}$	& GRN	& SARSA	& GRN$_{m}$	& GRN$_{g}$ 	\\\hline
avg		& \textbf{170.5} & 294.2 & \emph{225.3} & 234.1* & \emph{56.8} & \textbf{53.2} & 78.4*	& \textbf{639.6*}	& 639.0	&		&& \textbf{420.1*}	& 231.86	& \emph{320.0}& \\
stdev	& \emph{16.7} & \textbf{4.1} & 17.2 & 81.7*	  	& \emph{6.1}	& \textbf{4.3}	& 6.4* 		& \textbf{146.0*}	& 234.7	&		&& \textbf{14.7*}	& 127.9	& \emph{37.2} &\\
best		& \textbf{152.3} & 286.0 & \emph{191.0} & 199.6* & \emph{44.0} & \textbf{43.7} & 65.8*	& 744.2*& \textbf{907.1}	&		&& \emph{453.2*}	& \textbf{555.0}	& 376.7& \\
worst	&\textbf{ 227.4} & 308.1 & \emph{269.6} &1028.6*& \emph{73.2} & \textbf{73.1} & 78.0*	& \textbf{319.3*}	& -49.5	&		&& \textbf{382.2*}	& 93.5	& \emph{156.8}& \\\hline
\end{tabular}
\caption{Detailed results of the 100 runs done on the four problems with the best GRN specifically trained on the problem, with SARSA with no neuromodulation, with the GRN trained on the maze (noted GRN$_{m}$ in the table), and a generic GRN trained both on the maze and the mountain car problems (noted GRN$_{g}$). Notice that, for the first two problems (mountain car and maze), lower is better and for the two others (puddle world and actor critic pendulum), higher is better. Bold values are best per problem and per row and italic ones are second best.}\label{tab:results}
\end{table*}

One of the main property of the gene regulatory network is its capacity to generalize its behavior to unknown situations \cite{sanchez2014gene}. In this neuromodulation problem, we noticed that the best GRN trained on the maze problem and used on other problems good results in comparison to SARSA with no neuromodulation. As presented in table \ref{tab:results}, the GRN trained on maze (noted GRN$_{m}$) obtains better results than parameter-fixed SARSA on all problems we have tested. In our opinion, the regulation dynamics are very generic: when the rewards start to increase, lowering the learning rate to enter an exploitation phase seams appropriate to most problems.

However, this generic regulatory network does not beat GRNs specificially trained on the problems. This can be explained by the fact that optimizing the GRN specifically on a problem allows the system to find particular artifacts and thus improves the quality of the parameter regulation as well as the best initial values.

Figure \ref{fig:all:GRNMazeBehavior} shows the behavior of the GRN trained on the maze and used on other problems. The behavior are slightly different on each problem: the GRN is able to adapt to the problem specificities. In particular, when the episodes are taking to much time, this GRN increases $\lambda$ first and then $\alpha$. It is also interesting to notice that the stabilized values of the learning parameters different from a problem to another. For example, $\lambda$ is kept around 0.8 on the mountain car problem, around 0.65 for the puddle world problem and the actor critic pendulum whereas it is equal to 0.7 on the maze problem. It shows the capacity of the GRN to approximate the best learning parameters to a problem without further learning.

\begin{figure*}[t!]
\center
\begin{tabular}{ccc}
\includegraphics[width=0.32\linewidth]{MC_GRNMazeBehavior.pdf} &
\includegraphics[width=0.32\linewidth]{PW_GRNMazeBehavior.pdf} &
\includegraphics[width=0.32\linewidth]{ACP_GRNMazeBehavior.pdf} \\
(a) Mountain Car & (b) Puddle world & (c) Actor critic pendulum 
\end{tabular}
\caption{Behavior of the GRN trained on the maze when used to regulate learning parameters on mountain car, puddle world and actor critic pendulum.}\label{fig:all:GRNMazeBehavior}
\end{figure*}

Based on this observation, we have trained a new GRN on two problems instead of only one. The aim is to obtain a more generic GRN that have seen two very different problems. Therefore, we have chosen to train the GRN on the maze and the mountain car problems. !!! waiting for results !!!


Generalizing from which problems to which other problems. Does training on stochastic and testing on deterministic lead to better GRNs than training on
deterministic then testing on stochastic?
